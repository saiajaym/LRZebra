# -*- coding: utf-8 -*-
"""LRzebra.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K3vL9au0KRuQv6TuWq2UzIk5eW6s3kGm
"""

from google.colab import drive
drive.mount('/content/gdrive')
!ls -l /content/gdrive/'My Drive'/'Colab Notebooks'/data/images/pro |wc -l

# Commented out IPython magic to ensure Python compatibility.
#!pip install tensorflow --upgrade
from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler
from torchvision import transforms
from torch import nn
import torch
import pandas as pd
import os
import os.path
from PIL import Image
from torchvision import models
from tensorflow import summary
import datetime
import matplotlib.pyplot as plt
import numpy as np
import torchsummary
import datetime
# %load_ext tensorboard

current_time = str(datetime.datetime.now().timestamp())
train_log_dir = 'logs/tensorboard/train/' + current_time
test_log_dir = 'logs/tensorboard/test/' + current_time
train_summary_writer = summary.create_file_writer(train_log_dir)
test_summary_writer = summary.create_file_writer(test_log_dir)

class ZebraDataset(Dataset):
  def __init__(self, path,transform = None):
    self.transform = transform
    self.path = path
    self.data = []
    onlyfiles = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]
    #print(len(onlyfiles))
    onlyfiles.sort()
    #len(onlyfiles)
    onlyfiles = onlyfiles[1:]
    #
    csv = pd.read_csv(path + "SightingData.csv")
    idx=0
    for f in onlyfiles:
      self.data.append([f,csv["flank"][idx]])
      idx-=-1
  
  def __len__(self):
    return len(self.data)

  def __getitem__(self,id):
    file = Image.open(self.path+self.data[id][0]).convert('RGB')
    direc = 0
    if self.data[id][1]=='right':
      direc = 1
    if self.transform:
      file = self.transform(file)
    return file, direc

p = "/content/gdrive/My Drive/Colab Notebooks/data/images/gray/"
trans = transforms.Compose(
   [ transforms.ToTensor(),
    transforms.Normalize([0.485], [0.229])]
    )

t = ZebraDataset(p, transform=trans)

t.__getitem__(38)[0].shape
def imshow(inp, title=None):
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  
imshow(t.__getitem__(38)[0])

shuffle     = True
batch_size  = 16
num_workers = 2
validation_split = .2
shuffle_dataset = True
random_seed= 47

# Creating data indices for training and validation splits:
dataset_size = len(t)
indices = list(range(dataset_size))
split = int(np.floor(validation_split * dataset_size))
if shuffle_dataset :
    np.random.seed(random_seed)
    np.random.shuffle(indices)
train_indices, val_indices = indices[split:], indices[:split]

# Creating PT data samplers and loaders:
train_sampler = SubsetRandomSampler(train_indices)
valid_sampler = SubsetRandomSampler(val_indices)
trainl  = DataLoader(dataset=t, 
                         #shuffle=shuffle, 
                         batch_size=batch_size, 
                         num_workers=num_workers, pin_memory = True, sampler = train_sampler)
testl =  DataLoader(dataset=t, 
                         #shuffle=shuffle, 
                         batch_size=batch_size, 
                         num_workers=num_workers, pin_memory = True, sampler = valid_sampler)
tmp = next(iter(dataloader))
len(tmp[1])
tmp[0].shape

tmp = next(iter(trainl))
len(tmp[1])
tmp[0].shape

def get_trainable(model_params):
    return (p for p in model_params if p.requires_grad)


def get_frozen(model_params):
    return (p for p in model_params if not p.requires_grad)


def all_trainable(model_params):
    return all(p.requires_grad for p in model_params)


def all_frozen(model_params):
    return all(not p.requires_grad for p in model_params)

all_frozen(model.parameters())
def freeze_all(model_params):
    for param in model_params:
        param.requires_grad = False
model = models.resnet34(pretrained=True).to('cuda')
torchsummary.summary(res, (3, 256, 256), device="cuda")
res

for param in res.parameters():
    param.requires_grad = False
freeze_all(res.parameters())
assert all_frozen(res.parameters())

n_classes = 2
model.fc = nn.Linear(512, n_classes)

all_frozen(model.parameters())
criterion = nn.CrossEntropyLoss()

optimizer = torch.optim.Adam(
    get_trainable(model.parameters()),
    lr=0.001,
    # momentum=0.9,
)
DEVICE = 'cuda'
model = model.to(DEVICE)

N_EPOCHS = 16

for epoch in range(N_EPOCHS):
    
    # Train
    model.train()  # IMPORTANT
    
    total_loss, n_correct, n_samples = 0.0, 0, 0
    for batch_i, (X, y) in enumerate(trainl):
        X, y = X.to(DEVICE), y.to(DEVICE)
        
        optimizer.zero_grad()
        y_ = model(X)
        loss = criterion(y_, y)
        loss.backward()
        optimizer.step()
        
        # Statistics
        # print(
        #     f"Epoch {epoch+1}/{N_EPOCHS} |"
        #     f"  batch: {batch_i} |"
        #     f"  batch loss:   {loss.item():0.3f}"
        #)
        _, y_label_ = torch.max(y_, 1)
        n_correct += (y_label_ == y).sum().item()
        total_loss += loss.item() * X.shape[0]
        n_samples += X.shape[0]
    
    print(
        f"Epoch {epoch+1}/{N_EPOCHS} |"
        f"  train loss: {total_loss / n_samples:9.3f} |"
        f"  train acc:  {n_correct / n_samples * 100:9.3f}%"
    )
    
    
    # Eval
    model.eval()  # IMPORTANT
    
    total_loss, n_correct, n_samples = 0.0, 0, 0
    with torch.no_grad():  # IMPORTANT
        for X, y in testl:
            X, y = X.to(DEVICE), y.to(DEVICE)
                    
            y_ = model(X)
        
            # Statistics
            _, y_label_ = torch.max(y_, 1)
            n_correct += (y_label_ == y).sum().item()
            loss = criterion(y_, y)
            total_loss += loss.item() * X.shape[0]
            n_samples += X.shape[0]

    
    print(
        f"Epoch {epoch+1}/{N_EPOCHS} |"
        f"  valid loss: {total_loss / n_samples:9.3f} |"
        f"  valid acc:  {n_correct / n_samples * 100:9.3f}%"
    )

t = next(iter(testl))
#model.eval()

_,a = torch.max(model(t[0].to(DEVICE)),1)
model(t[0].to(DEVICE))
a

t[1]

for X, y in testl:
            X, y = X.to(DEVICE), y.to(DEVICE)
                    
            y_ = model(X)
        
            # Statistics
            _, y_label_ = torch.max(y_, 1)
            n_correct += (y_label_ == y).sum().item()
            loss = criterion(y_, y)
            total_loss += loss.item() * X.shape[0]
            n_samples += X.shape[0]
            print(
        f"  valid loss: {total_loss / n_samples:9.3f} |"
        f"  valid acc:  {n_correct / n_samples * 100:9.3f}%"
    )

